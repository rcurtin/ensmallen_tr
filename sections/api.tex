\section{API overview and examples}
\label{sec:api}

\begin{table}[t!]
\centering
    \begin{tabular}{@{} cl*{9}c @{}}
%  \begin{tabular}{ccccccc}
        & & \multicolumn{7}{c}{} \\[0.6ex]
            % If there is any coherent framework at all, this is true.
        & & \rot{unified framework}
            % If there is any support for constrained optimization, this is
            % true.
          & \rot{constraints}
            % If the optimization framework can do mini-batch, this is true.
          & \rot{separable functions / batches}
            % If I can implement any arbitrary function to be optimized, this is
            % true.
          & \rot{arbitrary functions}
            % If I can implement any new optimization technique to use, this is
            % true.
          & \rot{arbitrary optimizers}
            % If the framework could take advantage of when the gradient is
            % sparse, this is true.
          & \rot{sparse gradients}
            % If the framework can handle categorical/discrete variables for
            % optimization, this is true.
          & \rot{categorical}
            % If any type can be optimized, this is true.
          & \rot{arbitrary types}
            % If callback support is available.
          & \rot{callbacks} \\
        \cmidrule[1pt]{2-11}
        % It might be reasonable to say mlpack categorical support is only
        % partial, but I am not sure exactly where we draw the line.
        & \texttt{\small ensmallen}            & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE\\
        % The Shogun toolbox has a fairly nice framework, but it doesn't support
        % sparse gradients or categorical features.  It also does not appear to
        % support constraints, arbitrary types, or callbacks.
        & Shogun \cite{sonnenburg2010shogun}             & \CIRCLE & - & \CIRCLE
& \CIRCLE & \CIRCLE & - & - & - & - \\
        % VW doesn't appear to have any framework whatsoever and the code is
        % awful, but it does support batches and categorical features.
        & Vowpal Wabbit \cite{Langford2007VW}      & - & - & \CIRCLE  & - & - & - &
\CIRCLE & - & - \\
        % TensorFlow has a few optimizers, but they are all SGD-related.  You
        % can write most objectives easily (but some very hard), and categorical
        % support might be possible but would not be easy.
        & TensorFlow \cite{tensorflow2015-whitepaper}        & \CIRCLE & -  & \CIRCLE  & \LEFTcircle & - &
\LEFTcircle & - & \LEFTcircle & - \\
        % Caffe has a nice framework, but it's only for SGD-related optimizers.
        % I think I could write a new one, but it is not the easiest thing in
        % the world.
        & Caffe \cite{jia2014caffe}           & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - & \LEFTcircle & \CIRCLE \\
        % Keras is restricted to neural networks and SGD-like optimizers.  I
        % don't know that it is possible to easily write a new optimizer.
        & Keras \cite{chollet2015keras}            & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - & \LEFTcircle & \CIRCLE \\
        % sklearn has a few optimizer frameworks, but they are all in different
        % places and have somewhat different support.
        & scikit-learn \cite{pedregosa2011scikit}       & \LEFTcircle & - & \LEFTcircle  & \LEFTcircle & -
& - & - & \LEFTcircle & - \\
        % scipy has some nice optimizer framework but it does not support
        % batches or some of the more complex functionality.  And you can't
        % write your own.
        & SciPy \cite{jones2014scipy}             & \CIRCLE & \CIRCLE  & -  &
\CIRCLE & - & - & - & \LEFTcircle & \CIRCLE \\
        % MATLAB is very similar to scipy.
        & MATLAB \cite{matlab_fminsearch}            & \CIRCLE & \CIRCLE & - &
\CIRCLE & - & - & - & \LEFTcircle & - \\
        % Optim.jl isn't the only Julia package for optimization, but it's the
        % one we compare against.
        & Julia (\texttt{\small Optim.jl}) \cite{mogensen2018optim}         &
\CIRCLE & - & - & \CIRCLE & - & - & - & \CIRCLE & - \\
        \cmidrule[1pt]{2-11}
    \end{tabular}
\caption{\footnotesize{
Feature comparison: \CIRCLE = provides feature,
\LEFTcircle = partially provides feature, - = does not provide feature.
{\it unified framework} indicates if there is some kind of generic/unified
optimization framework; {\it constraints} and {\it separable functions /
batches} indicate support for constrained functions and separable functions;
{\it arbitrary functions} means arbitrary objective functions are easily
implemented; {\it arbitrary optimizers} means arbitrary optimizers are easily
implemented; {\it sparse gradient} indicates that the framework can natively
take advantage of sparse gradients; {\it categorical} refers to if support
for categorical features exists; {\it arbitrary types} mean that arbitrary types
can be used for the parameters $x$; and {\it callbacks} mean that
user-implementable callback support is available.
}}
\label{tab:comparison}
\end{table}

{\tt ensmallen} provides a {\bf set of optimizers} for optimizing {\bf
user-defined objective functions}.  These optimizers are generic and flexible,
meaning that they can support a wide range of use cases and applications.  A
primary differentiator from other toolkits is that this flexibility is provided
via template metaprogramming, which means that it comes at no runtime cost.
In addition to allowing users to easily define and optimize their own objective
functions, {\tt ensmallen} also makes it easy to implement new optimizers, which
can then be contributed back upstream and incorporated into the library.

Table~\ref{tab:comparison} compares the functionality
supported by {\tt ensmallen} and other optimization toolkits.

\subsection{Types of objective functions}

Overall, our primary goal is to provide an easy-to-use and efficient library
that can solve the problem $\operatorname{argmin}_x f(x)$ for any function
$f(x)$ that takes a vector or matrix input $x$.  In most cases, $f(x)$ will have
some structure; one simple example might be that $f(x)$ is differentiable.
Therefore, the abstractions that we have built for {\tt ensmallen} can
optionally take advantage of this structure.  In the example of a differentiable
function $f(x)$, the user can provide an implementation of the gradient $f'(x)$,
which in turn allows a first-order optimizer to be used.  This generally leads
to significant, order-of-magnitude speedups.

To capture this structure, {\tt ensmallen}'s optimizers are built to work with
different types of objective functions.  These classes of objectives functions
are listed below.

\begin{itemize}
\item {\bf Arbitrary functions} ({\tt \small ArbitraryFunctionType}).  No
assumptions can be made on an arbitrary function $f(x)$ and only the objective
$f(x)$ can be computed for a given $x$.

\item {\bf Differentiable functions} ({\tt \small DifferentiableFunctionType}).
A differentiable function $f(x)$ is an arbitrary function whose gradient $f'(x)$
can be computed for a given $x$, in addition to the objective.

\item {\bf Partially differentiable functions} ({\tt \small
PartiallyDifferentiableFunctionType}).  A partially differentiable function
$f(x)$ is a differentiable function with the additional property that the
gradient $f'(x)$ can be decomposed along some basis $j$ such that $f_j'(x)$ is
sparse.  Often, this is used for coordinate descent algorithms (i.e., $f'(x)$
can be decomposed into $f_{x1}'(x)$, $f_{x2}'(x)$, etc.).

\item {\bf Arbitrary separable functions} ({\tt \small
ArbitrarySeparableFunctionType}).  An arbitrary separable function is an
arbitrary function $f(x)$ that can be decomposed into the sum of several
objective functions:

\begin{equation}
f(x) = \sum_i f_i(x).
\end{equation}

\item {\bf Differentiable separable functions} ({\tt \small
DifferentiableSeparableFunctionType}).  A differentiable separable function is a
separable arbitrary function $f(x)$ where the individual gradients $f_i'(x)$ are
also computable.

\item {\bf Categorical functions} ({\tt \small CategoricalFunctionType}).  A
categorical function type is an arbitrary function $f(x)$ where some (or all)
dimensions of $x$ take discrete values from a set.

\item {\bf Constrained functions} ({\tt \small ConstrainedFunctionType}).  A
constrained function $f(x)$ is a differentiable function\footnote{In general, it
is not a requirement that a constrained function is differentiable.  But we
require it here, as all {\tt ensmallen}'s current optimizers for constrained
functions require a gradient to be available.} subject to constraints of the
form $c_i(x)$; when the constraints are satisfied, $c_i(x) = 0\; \forall \; i$.
Minimizing $f(x)$ then means minimizing $f(x) + \sum_i c_i(x)$.

\item {\bf Semidefinite programs} (SDPs).  {\it (These are a subset of
constrained functions.)}  {\tt ensmallen} has special
support to make optimizing semidefinite
programs~\cite{vandenberghe1996semidefinite} simple.
\end{itemize}

Details about how to implement and use each type of objective function can be
found on the {\tt ensmallen} website, at \url{https://ensmallen.org/docs.html},
but is omitted here for brevity (and because the software may change over
time!).

\subsection{Types of optimizers}

Using this straightforward abstraction framework, {\tt ensmallen} is able to
provide a wide range of optimizers:

\begin{itemize}
  \item {\bf For arbitrary functions.}  Simulated
annealing~\cite{kirkpatrick1983optimization}, CNE
(Conventional Neural Evolution)~\cite{montana1989training}, DE (Differential
Evolution)~\cite{storn1997differential}, PSO (Particle Swarm
Optimization)~\cite{Kennedy1995}, SPSA (Simultaneous Perturbation
Stochastic Approximation)~\cite{spall1992multivariate}.

  \item {\bf For differentiable functions.}  L-BFGS~\cite{liu1989limited},
Frank-Wolfe~\cite{jaggi2013revisiting}, gradient descent.

  \item {\bf For partially differentiable functions.}  SCD (Stochastic
Coordinate Descent)~\cite{Shalev-Shwartz2009}.

  \item {\bf For arbitrary separable functions.}  CMA-ES (Covariance Matrix
Adaptation Evolution Strategy)~\cite{Hansen2001}.

  \item {\bf For differentiable separable functions.}
AdaBound~\cite{Luo2019AdaBound},
AdaDelta~\cite{zeiler2012adadelta}, AdaGrad~\cite{duchi2011adaptive},
Adam~\cite{Kingma2014}, AdaMax~\cite{Kingma2014},
AMSBound~\cite{Luo2019AdaBound}, AMSGrad~\cite{reddi2019convergence},
Big Batch SGD~\cite{De2017}, Eve~\cite{Koushik2016}, FTML (Follow The Moving
Leader)~\cite{Zheng2017},
Hogwild!~\cite{recht2011hogwild}, IQN
(Incremental Quasi-Newton)~\cite{1106.5730}, Katyusha~\cite{Allen-Zhu2016},
Lookahead~\cite{Zhang2019}, SGD with momentum~\cite{rumelhart1988learning},
Nadam~\cite{Dozat2015},
NadaMax~\cite{Dozat2015}, SGD with Nesterov momentum~\cite{Nesterov1983},
Optimistic
Adam~\cite{daskalakis2017training}, QHAdam (Quasi-Hyperbolic
Adam)~\cite{ma2019qh}, QHSGD
(Quasi-Hyperbolic Stochastic Gradient Descent)~\cite{ma2019qh},
RMSProp~\cite{tieleman2012lecture},
SARAH/SARAH+~\cite{Nguyen2017}, stochastic gradient descent, SGDR (Stochastic Gradient
Descent with Restarts)~\cite{Loshchilov2016}, Snapshot SGDR~\cite{Huang2017},
SMORMS3~\cite{Funk2015}, SVRG (Stochastic Variance Reduced
Gradient)~\cite{Johnson2013}, SWATS~\cite{Keskar2017},
SPALeRA (Safe Parameter-wise Agnostic LEarning Rate
Adaptation)~\cite{Schoenauer2017},
WNGrad~\cite{Wu2018}.

  \item {\bf For categorical functions.}  Grid search.

  \item {\bf For constrained functions.}  Augmented Lagrangian method,
primal-dual interior point SDP solver, LRSDP (low-rank accelerated SDP
solver)~\cite{burer2003nonlinear}.
\end{itemize}

\subsection{Using {\tt ensmallen}}

The task of optimizing an objective function with {\tt ensmallen} is
straightforward.  The class of objective function (e.g., arbitrary, constrained,
differentiable, etc.) defines the implementation requirements.  For instance, if
a user wants to optimize some objective function $f(x)$ that is differentiable,
they merely need to provide an implementation of $f(x)$ and $f'(x)$ and then
they can immediately use one of {\tt ensmallen}'s optimizers for differentiable
functions.  That is, each objective function type has some minimal set of
methods that must be implemented.  Typically this is only between one and four
methods.

Note that not every type of objective function can be used with every type of
optimizer.  For instance, {\tt L\_BFGS} is a differentiable optimizer, and so it
cannot be used with any non-differentiable object function type (e.g. an
arbitrary function).  However, {\tt ensmallen} still allows as much flexibility
as possible via template metaprogramming:

\begin{enumerate}
  \item When an optimizer is used with a user-provided objective function,
template metaprogramming is used along with static assertions to provide
user-friendly error messages if any required methods are not detected.

  \item When possible, {\tt ensmallen} will automatically infer methods that are
not provided.  For instance, given a separable objective function where an
implementation of $f_i(x)$ is provided (as well as the number of such separable
objectives), an implementation of $f(x)$ can be inferred.  This is all done at
compile-time, and so there is no additional runtime overhead compared to a
handwritten implementation.
\end{enumerate}

To give an example, consider the example of linear regression, where we are
given a matrix of predictors $\bm X \in \mathcal{R}^{n \times d}$ and a vector
of responses $\bm y \in \mathcal{R}^n$.  Our task is to find the best linear
model $\bm \theta \in \mathcal{R}^d$; that is, we want to find $\bm \theta^* =
\operatorname{argmin} f(\bm \theta)$ for
\begin{equation}
f(\bm \theta) = \| \bm X \bm \theta - \bm y \|^2 = (\bm X \bm \theta - \bm y)^T
(\bm X \bm \theta - \bm y).
\label{eqn:obj_lr}
\end{equation}
From this we can easily derive the gradient $f'(\bm \theta)$:
\begin{equation}
f'(\bm \theta) = 2 \bm X^T (\bm X \bm \theta - \bm y).
\label{eqn:grad_lr}
\end{equation}

If we want to use {\tt ensmallen} to find $\bm \theta^*$ using a differentiable
optimizer, we simply need to provide implementations of $f(\bm \theta)$ and
$f'(\bm \theta)$ according to the signatures required by the {\tt
DifferentiableFunctionType} of objective function.  For a differentiable
function, only two methods are necessary: {\tt Evaluate()} and {\tt Gradient()}.
The introductory discussion here does not go into the full details of the
different signatures required for each objective function; for that, see the
{\tt ensmallen} documentation: \url{https://ensmallen.org/docs.html}.

Returning to our linear regression example, we could easily use {\tt ensmallen}'s
L-BFGS implementation to find $\bm \theta^*$; we just need to provide an
implementation of $f(\bm \theta)$ and $f'(\bm \theta)$ (as L-BFGS requires a
differentiable objective function).  Figure~\ref{fig:lr_function} is an example
implementation.

\begin{figure}[t!]
\hrule
\vspace{1ex}
\begin{minted}{c++}
class LinearRegressionFunction
{
 public:
  LinearRegressionFunction(const arma::mat& X, const arma::vec& y) : X(X), y(y)
  { }

  double Evaluate(const arma::mat& coordinates)
  {
    return (X * coordinates - y).t() * (X * coordinates - y);
  }

  void Gradient(const arma::mat& coordinates, arma::mat& gradient)
  {
    gradient = 2 * X.t() * (X * coordinates - y);
  }

  // Note that ensmallen gives us the option of only implementing *one* function
  // EvaluateWithGradient() instead, which would be more efficient in this case
  // since it can share computation between Evaluate() and Gradient()!
  //
  // (We use both Evaluate() and Gradient() here for simplicity of exposition.)

 private:
  const arma::mat& X;
  const arma::vec& y;
};
\end{minted}
\hrule
\vspace*{-0.5em}
\caption{An example implementation of an objective function class for linear
regression.}
\label{fig:lr_function}
\end{figure}
\begin{figure}[t!]

\hrule
\vspace{1ex}
\begin{minted}{c++}
// We assume that "X" and "y" are given.
LinearRegressionFunction f(X, y);

L_BFGS optimizer; // Create the optimizer with all default parameters.

// The theta_best matrix will hold the best model that we find after we call
// Optimize(); for now, we set it to the initial point (uniform random values).
arma::mat theta_best(X.n_rows, 1, arma::fill:randu);

optimizer.Optimize(f, theta_best);
\end{minted}
\hrule
\vspace*{-0.5em}
\caption{Example usage of {\tt ensmallen}'s L-BFGS optimizer to optimize the
linear regression objective function.}
\label{fig:lbfgs_lr_opt}
\end{figure}

In this example, we hold {\tt \small X} and {\tt \small y} as members of the
class, and {\tt \small coordinates} is used to represent $\bm \theta$.  Via the
use of Armadillo~\cite{sanderson2016armadillo}, the linear algebra expressions
to implement the objective function and gradient are readable in a way that
closely matches Equations~\ref{eqn:obj_lr} and \ref{eqn:grad_lr}.  With {\tt
\small LinearRegressionFunction} implemented, we can easily find $\bm \theta^*$
with the code in Figure~\ref{fig:lbfgs_lr_opt} that uses {\tt ensmallen}'s
L-BFGS optimizer.

However, it's possible to do more than optimize a model that's stored as an {\tt
\small arma::mat} (dense matrix).  {\tt ensmallen} supports:

\begin{itemize}
  \item Use of types other than {\tt \small arma::mat} when calling {\tt \small
Optimize()}.  This can include integer-valued matrices, sparse matrices, or any
type whose implementation matches the Armadillo API.

  \item Callbacks, which can specify custom behavior during the optimization.
Examples include printing the loss function value at each iteration, or
terminating when a time budget is used up, or other custom user-supplied
functionality.

  \item Inference of functions not supplied by the user.  One example is that
for a differentiable function, a user might supply a {\bf joint} implementation
of the {\tt Gradient()} and {\tt Evaluate()} methods, instead of implementing
them separately.  {\tt ensmallen} can automatically infer variants of {\tt
Gradient()} and {\tt Evaluate()} when needed.
\end{itemize}

Each of the following sections will discuss how we are able to achieve the
functionality above---with no runtime overhead.
