\section{Introduction}
\label{sec:introduction}

The problem of mathematical optimization is fundamental in the computational
sciences.  In short, this problem is expressed as

\begin{equation}
\operatorname{argmin}_x f(x).
\end{equation}

The ubiquity of this problem gives rise to the proliferation of mathematical
optimization toolkits: SciPy~\cite{2019arXiv190710121V},
opt++~\cite{meza1994opt++},
OR-Tools~\cite{ortools}, CVXOPT~\cite{vandenberghe2010cvxopt},
NLopt~\cite{johnson2014nlopt}, Ceres~\cite{ceres-solver},
and RBFOpt~\cite{costa2018rbfopt} are just a few examples of software dedicated
solely to optimization.  On top of that, in the field of machine learning, many
deep learning libraries have integrated optimization
components---for example, Theano~\cite{2016arXiv160502688full},
TensorFlow~\cite{tensorflow2015-whitepaper}, PyTorch~\cite{NEURIPS2019_9015},
and Caffe~\cite{jia2014caffe} all have integrated mathematical optimization
components.

However, these optimization routines are generally quite computationally
intensive: for instance, the training of deep neural networks is dominated by
the optimization of the model parameters on the
data~\cite{krizhevsky2012imagenet, lauzon2012introduction}.  Similarly,
other popular machine learning algorithms such as logistic regression are also
expressed as and dominated by an optimization~\cite{zhang2004solving,
manogaran2018health}.  Computational bottlenecks occur even in fields as
wide-ranging as rocket landing guidance systems~\cite{dueri2016customized},
motivating the development and implementation of specialized solvers.

Yet, many general-purpose optimization libraries fail to make full use of the
hardware on which they are running.  Often, in order to make a clean and
easy-to-use API, general-purpose optimization libraries use design patterns that
implicitly make performance compromises.  For instance, SciPy's {\tt minimize()}
method takes the function {\tt fun} to be optimized as a parameter; but in
Python, this will mean that every time {\tt fun} is called (e.g., to compute the
value of $f(x)$ during the optimization routine), there will be overhead
involved---it will be slower than if the optimization routine were written {\it
specifically} for the function {\tt fun}.

It is inefficiencies such as these that motivated us to implement the {\tt
ensmallen} library in C++, originally as a part of the {\tt mlpack} machine
learning library~\cite{mlpack2018}.  {\tt ensmallen} uses template
metaprogramming techniques to produce efficient code during compilation; through
these techniques, we are able to simultaneously provide a friendly and intuitive
interface that matches the ease of use of popular optimization toolkits like
SciPy, OR-Tools, and MATLAB's {\tt fminsearch()}
function~\cite{matlab_fminsearch}.

% the 46 optimizers are as of 2.11.0
{\tt ensmallen} allows the user to define a function $f(x)$ to be optimized with
a wide variety of optimization techniques (at the time of this writing, 46
optimizers are available).  Many of these optimization techniques operate on
specific {\it classes} of objective functions---for instance, {\tt ensmallen}
contains many optimizers that work on differentiable functions, and so a user
must provide both $f(x)$ and $f'(x)$ in this case.  Via templates, {\tt
ensmallen} allows the user to select an arbitrary type for $x$ (e.g., dense
floating-point matrix, sparse integer matrix, etc.).  Users may also specify
custom behavior via customizable {\it callbacks}.  It is also easy to implement
new optimization techniques inside of the framework {\tt ensmallen} provides.

In this paper, we describe the details of these template metaprogramming
techniques and how we are able to simultaneously produce efficient code and a
clean user interface.  In Section~\ref{sec:api}, we introduce the functionality
and interface for {\tt ensmallen} with examples.
Section~\ref{sec:templated_optimize} is our first foray into template
metaprogramming, showing how {\tt ensmallen} can optimize objective functions
$f(x)$ for any type $x$ without any extra runtime overhead.  Then,
Section~\ref{sec:automatic} discusses the details of how {\tt ensmallen} is able
to automatically generate methods not provided by the user.
Section~\ref{sec:callbacks} describes {\tt ensmallen}'s callbacks.
Finally, we demonstrate the empirical efficiency of {\tt ensmallen} in
Section~\ref{sec:experiments} and conclude in Section~\ref{sec:conclusion}.

This paper is an expanded description of {\tt ensmallen}'s original
introduction~\cite{ensmallen2018, curtin2017generic}, and provides a deep dive
into the internals of how the library works.  This means it can be a very useful
resource for anyone looking to contribute to the library or get involved with
its development.
