\section{Introduction}
\label{sec:introduction}

The problem of mathematical optimization is fundamental in the computational
sciences.  In short, this problem is expressed as
%
\begin{equation}
\operatornamewithlimits{argmin}_x f(x)
\end{equation}

\noindent
where $f(x)$ is a given objective function and $x$ is typically a vector or matrix.
The ubiquity of this problem gives rise to the proliferation of mathematical
optimization toolkits, such as SciPy~\cite{2019arXiv190710121V},
opt++~\cite{meza1994opt++},
OR-Tools~\cite{ortools}, CVXOPT~\cite{vandenberghe2010cvxopt},
NLopt~\cite{johnson2014nlopt}, Ceres~\cite{ceres-solver},
%% {\tt fminsearch()} in MATLAB~\cite{matlab_fminsearch},
and RBFOpt~\cite{costa2018rbfopt}.
Furthermore, in the field of machine learning, many
deep learning frameworks have integrated optimization
components.  Examples include Theano~\cite{2016arXiv160502688},
TensorFlow~\cite{tensorflow2015-whitepaper}, PyTorch~\cite{NEURIPS2019_9015},
and Caffe~\cite{jia2014caffe}.

Mathematical optimization is generally quite computationally intensive.
For instance, the training of deep neural networks is dominated by
the optimization of the model parameters on the
data~\cite{krizhevsky2012imagenet, lauzon2012introduction}.  Similarly,
other popular machine learning algorithms such as logistic regression are also
expressed as and dominated by an optimization process~\cite{zhang2004solving,
manogaran2018health}.  Computational bottlenecks occur even in fields as
wide-ranging as rocket landing guidance systems~\cite{dueri2016customized},
motivating the development and implementation of specialized solvers.

The necessity for both efficient and specializable function optimization motivated us
to implement the {\tt ensmallen} library, originally as part of the {\tt mlpack}
machine learning library~\cite{mlpack2018}.
The {\tt ensmallen} library provides a large {set of pre-built optimizers} for optimizing
{user-defined objective functions} in C++;
at the time of writing, 46 optimizers are available.  
The external interface to the optimizers is intuitive and matches the ease of use of popular
optimization toolkits mentioned above.

However, unlike many of the existing optimization toolkits,
{\tt ensmallen} explicitly supports numerous function types:
arbitrary, differentiable, separable, categorical, constrained, and semidefinite.
Furthermore, custom behavior during optimization can be easily specified via {\it callbacks}.
Lastly, the underlying framework facilitates the implementation of new optimization techniques,
which can be contributed upstream and incorporated into the library.
Table~\ref{tab:comparison} contrasts the functionality supported
by {\tt ensmallen} and other optimization toolkits.

This paper is a revised and expanded description of our initial overview
of {\tt ensmallen}~\cite{ensmallen2018}. 
It also provides a deep dive into the internals
of how the library works, which can be a useful resource for anyone looking
to contribute to the library or get involved with its development.

%% TODO: this will need to be refactored once the document is refactored
% In this paper, we describe the details of the template metaprogramming
% techniques and how we are able to simultaneously produce efficient code and a
% clean user interface.

\TODO{update}
The paper is continued as follows.
We introduce the functionality and interface for {\tt ensmallen} in Section~\ref{sec:api}.
Section~\ref{sec:templated_optimize} shows how {\tt ensmallen} can optimize objective functions
$f(x)$ for various types of $x$ without any extra runtime overhead.
Section~\ref{sec:automatic} discusses the details of how {\tt ensmallen} is able
to automatically generate methods not provided by the user.
Section~\ref{sec:callbacks} describes {\tt ensmallen}'s callbacks.
We demonstrate the empirical efficiency of {\tt ensmallen} in
Section~\ref{sec:experiments} and conclude in Section~\ref{sec:conclusion}.


\begin{table}[!t]
\centering
    \begin{tabular}{@{} cl*{9}c @{}}
%  \begin{tabular}{ccccccc}
        & & \multicolumn{7}{c}{} \\[0.6ex]
            % If there is any coherent framework at all, this is true.
        & & \rot{unified framework}
            % If there is any support for constrained optimization, this is
            % true.
          & \rot{constraints}
            % If the optimization framework can do mini-batch, this is true.
          & \rot{separable functions / batches}
            % If I can implement any arbitrary function to be optimized, this is
            % true.
          & \rot{arbitrary functions}
            % If I can implement any new optimization technique to use, this is
            % true.
          & \rot{arbitrary optimizers}
            % If the framework could take advantage of when the gradient is
            % sparse, this is true.
          & \rot{sparse gradients}
            % If the framework can handle categorical/discrete variables for
            % optimization, this is true.
          & \rot{categorical}
            % If any type can be optimized, this is true.mention
          & \rot{arbitrary types}
            % If callback support is available.
          & \rot{callbacks} \\
        \cmidrule[1pt]{2-11}
        % It might be reasonable to say mlpack categorical support is only
        % partial, but I am not sure exactly where we draw the line.
        & \texttt{ensmallen}            & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE & \CIRCLE\\
        % The Shogun toolbox has a fairly nice framework, but it doesn't support
        % sparse gradients or categorical features.  It also does not appear to
        % support constraints, arbitrary types, or callbacks.
        & Shogun \cite{sonnenburg2010shogun}             & \CIRCLE & - & \CIRCLE
& \CIRCLE & \CIRCLE & - & - & - & - \\
        % VW doesn't appear to have any framework whatsoever and the code is
        % awful, but it does support batches and categorical features.
        & Vowpal Wabbit \cite{Langford2007VW}      & - & - & \CIRCLE  & - & - & - &
\CIRCLE & - & - \\
        % TensorFlow has a few optimizers, but they are all SGD-related.  You
        % can write most objectives easily (but some very hard), and categorical
        % support might be possible but would not be easy.
        & TensorFlow \cite{tensorflow2015-whitepaper}        & \CIRCLE & -  & \CIRCLE  & \LEFTcircle & - &
\LEFTcircle & - & \LEFTcircle & - \\
        % Caffe has a nice framework, but it's only for SGD-related optimizers.
        % I think I could write a new one, but it is not the easiest thing in
        % the world.
        & Caffe \cite{jia2014caffe}           & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - & \LEFTcircle & \CIRCLE \\
        % Keras is restricted to neural networks and SGD-like optimizers.  I
        % don't know that it is possible to easily write a new optimizer.
        & Keras \cite{chollet2015keras}            & \CIRCLE & -  & \CIRCLE & \LEFTcircle & \LEFTcircle
& - & - & \LEFTcircle & \CIRCLE \\
        % sklearn has a few optimizer frameworks, but they are all in different
        % places and have somewhat different support.
        & scikit-learn \cite{pedregosa2011scikit}       & \LEFTcircle & - & \LEFTcircle  & \LEFTcircle & -
& - & - & \LEFTcircle & - \\
        % scipy has some nice optimizer framework but it does not support
        % batches or some of the more complex functionality.  And you can't
        % write your own.
        & SciPy \cite{jones2014scipy}             & \CIRCLE & \CIRCLE  & -  &
\CIRCLE & - & - & - & \LEFTcircle & \CIRCLE \\
        % MATLAB is very similar to scipy.
        & MATLAB \cite{matlab_fminsearch}            & \CIRCLE & \CIRCLE & - &
\CIRCLE & - & - & - & \LEFTcircle & - \\
        % Optim.jl isn't the only Julia package for optimization, but it's the
        % one we compare against.
        & Julia (\texttt{Optim.jl}) \cite{mogensen2018optim}         &
\CIRCLE & - & - & \CIRCLE & - & - & - & \CIRCLE & - \\
        \cmidrule[1pt]{2-11}
    \end{tabular}
\caption{
Feature comparison: \CIRCLE~= provides feature,
\LEFTcircle~= partially provides feature, -~= does not provide feature.
{\it unified framework} indicates if there is a form of generic/unified
optimization framework; {\it constraints} and {\it separable functions /
batches} indicate support for constrained functions and separable functions;
{\it arbitrary functions} means arbitrary objective functions are easily
implemented; {\it arbitrary optimizers} means arbitrary optimizers are easily
implemented; {\it sparse gradient} indicates that the framework can natively
take advantage of sparse gradients; {\it categorical} refers to if support
for categorical features exists; {\it arbitrary types} mean that arbitrary types
can be used for the parameters $x$;
{\it callbacks} indicates that user-implementable callback support is available.
%\TODO{may want to add a comparison with PyTorch, as that's getting popular these days}
}
\label{tab:comparison}
\vspace{1ex}
\hrule
\end{table}

