% TODO: this section name isn't particularly great...
\section{Automatic function generation}
\label{sec:automatic}

In the previous section, we saw an example of how a user might implement {\tt
Evaluate()} and {\tt Gradient()} for the linear regression objective function
and use {\tt ensmallen} to find the minimum.  But there is a clear inefficiency:
the objective function computation $f(\bm \theta)$ is defined as

\begin{equation*}
\| \bm X \bm \theta - \bm y \|^2
\end{equation*}

\noindent and the gradient computation $f'(\bm \theta)$ is defined as

\begin{equation}
2 \bm X^T (\bm X \bm \theta - \bm y).
\end{equation}

There is a shared inner computation in $f(\bm \theta)$ and $f'(\bm \theta)$: the
term $(\bm X \bm \theta - \bm y)$.  But if we implement $f(\bm \theta)$ and
$f'(\bm \theta)$ as separate functions, there is no easy way to exploit this
shared computation: {\tt ensmallen}'s differentiable optimizers treat the
functions they are given as oracular, and cannot and do not know anything about
the internal computations of those functions.  In fact, this inefficiency
necessarily applies to any optimization package that accepts an objective
function and its gradient as separate parameters; that includes SciPy, {\tt
bfgsmin()}, {\tt Optim.jl}, and other packages.\footnote{It could be possible
for an autodifferentiation package to avoid this efficiency, or a
programming language with introspection to operate directly on the AST of the
given objective and gradient computations to successfully share the computation.
However, at the time of this writing, we are not aware of any that do this, and
that seems a difficult engineering task.}

To work around this issue, we use template metaprogramming techniques to allow
the user to provide {\it either} separate implementations of the objective
function and gradient, {\it or} a combined implementation that computes {\it
both} the objective function and gradient simultaneously (thus allowing the
sharing of inner computations).  That is, the user can provide the methods {\tt
Evaluate()} and {\tt Gradient()}, or {\tt EvaluateWithGradient()}.  (They can
provide both, if they so choose.)  For the example objective function above, we
will see in our experiments that the ability to provide {\tt
EvaluateWithGradient()} can result in a significant speedup.

Similarly, when implementing a differentiable optimizer in {\tt ensmallen}, it
is possible to use {\it either} {\tt Evaluate()} and {\tt Gradient()} {\it or}
{\tt EvaluateWithGradient()} (or both) during optimization.
In fact, this same technique can be used to infer and provide missing methods
for separable functions, differentiable separable functions, constrained
functions, categorical functions, and others.  Not all of these possibilities
are currently implemented in {\tt ensmallen}, but the framework described below
makes it straightforward to add more.  We will describe the framework in a
simplified form, focusing only the {\tt EvaluateWithGradient()}/{\tt
Evaluate()}/{\tt Gradient()} example described above.

Using our framework for function inference is very simple; an example
differentiable optimizer might look like this:

\begin{minted}{c++}
// FunctionType is the user-supplied function type to optimize.
// MatType is the user-supplied type of the initial coordinates.
// GradType is the user-specified type of the gradient.
// typename MatType::elem_type represents the internal type held by MatType
//     (e.g., if MatType is `arma::mat`, then this is `double`)
template<typename FunctionType, typename MatType, typename GradType>
typename MatType::elem_type Optimize(FunctionType& function,
                                     MatType& coordinates)
{
  // The Function<> mix-in class adds all inferrable methods to `function`
  // So, if `function` has `function.Evaluate()` and `function.Gradient()`, then
  // `fullFunction` will have `fullFunction.EvaluateWithGradient()`.
  typename Function<FunctionType, MatType, GradType> fullFunction(function);

  // The rest of the optimizer's code should use `fullFunction`, not `function`.
  ...
}
\end{minted}

This is relatively uninvasive: {\tt ensmallen} contributors who are writing
optimizers need only define a {\tt Function<FunctionType, ...>} wrapper at the
beginning of optimization, and can then expect all three of {\tt Evaluate()},
{\tt Gradient()}, and {\tt EvaluateWithGradient()} to be available.  Users who
want to optimize objective functions with {\tt ensmallen} do not even need to
think about it.

The class {\tt Function<...>} is like a {\it mixin}
class~\cite{smaragdakis2000mixin}; more accurately, it is a {\it collection} of
mixin classes.  Below is a shortened snippet of the definition of the {\tt
Function} class:

\begin{minted}{c++}
template<typename FunctionType, typename MatType, typename GradType>
class Function :
    ... // lots of other mixin classes omitted
    public AddEvaluateWithGradient<FunctionType, MatType, GradType>,
    ...
    public FunctionType
\end{minted}

We can see that {\tt Function} inherits methods from the given {\tt
FunctionType} (which is the user-supplied objective function class that is to be
optimized), and also from the {\tt AddEvaluateWithGradient<...>} mixin class.
The key to our approach is that if {\tt FunctionType} class has an {\tt
EvaluateWithGradient()} method, then {\tt AddEvaluateWithGradient<...>} will
provide no functions; if {\tt FunctionType} does {\em not} have an {\tt
EvaluateWithGradient()} method, then {\tt AddEvaluateWithGradient<...>} will
provide an {\tt EvaluateWithGradient()} function.

The details of how this work depend on the SFINAE technique~\cite{TODO} and
template specialization.  The {\tt AddEvaluateWithGradient} class has five
template parameters; the last two of these have default values:

\begin{minted}{c++}
template<typename FunctionType,
         typename MatType,
         typename GradType,
         // Check if FunctionType has at least one non-const Evaluate() or
         // Gradient().
         bool HasEvaluateGradient = ...
         // Check if FunctionType has an EvaluateWithGradient() method already.
         bool HasEvaluateWithGradient = ...
class AddEvaluateWithGradient
{
 public:
  // Provide a dummy overload so the name 'EvaluateWithGradient' exists for this
  // object.
  typename MatType::elem_type EvaluateWithGradient(
      traits::UnconstructableType&);
};
\end{minted}

For the sake of brevity we omit the expressions for the {\tt bool} parameters
{\tt HasEvaluateGradient} and {\tt HasEvaluateWithGradient}\footnote{Interested
readers can find that code in {\tt
ensmallen\_bits/function/add\_evaluate\_with\_gradient.hpp}.}.  These two
parameters are traits expressions~\cite{TODO} that depend heavily on SFINAE
techniques for method detection; {\tt HasEvaluateGradient} will evaluate to {\tt
true} if {\tt FunctionType} has {\tt Evaluate()} and {\tt Gradient()}; if only
one (or neither) is available, the value is {\tt false}.  {\tt
HasEvaluateWithGradient} will evaluate to {\tt true} if {\tt FunctionType} has
an overload of {\tt EvaluateWithGradient()}, and {\tt false} otherwise.

Using these two boolean template variables, we can then use template
specialization to control the behavior of {\tt AddEvaluateWithGradient} as a
function of what is provided by {\tt FunctionType}.  Specifically, we make the
following two specializations:

\begin{minted}{c++}
template<typename FunctionType,
         typename MatType,
         typename GradType,
         bool HasEvaluateGradient>
class AddEvaluateWithGradient<FunctionType,
                              MatType,
                              GradType,
                              HasEvaluateGradient,
                              true>
{
 public:
  // Reflect the existing EvaluateWithGradient().
  typename MatType::elem_type EvaluateWithGradient(
      const MatType& coordinates, GradType& gradient)
  {
    return static_cast<FunctionType*>(
        static_cast<Function<FunctionType,
                             MatType,
                             GradType>*>(this))->EvaluateWithGradient(
        coordinates, gradient);
  }
};
\end{minted}

This specialization above is for when {\tt HasEvaluateWithGradient} is {\tt
true}---meaning that {\tt FunctionType} already has {\tt
EvaluateWithGradient()}.  In this situation, the implementation of {\tt
AddEvaluateWithGradient::EvaluateWithGradient()} simply calls out to {\tt
FunctionType::EvaluateWithGradient()}.  However, there is some complexity here,
as to successfully call {\tt FunctionType::EvaluateWithGradient()}, we must
first cast the {\tt this} pointer to have type {\tt FunctionType}---which can
only be done by first casting {\tt this} to the derived class {\tt
Function<...>}.  (This cast is only safe because we know that we will never
create an {\tt AddEvaluateWithGradient} outside of the context of the {\tt
Function<>} class.

Next, we specialize for the case where {\tt HasEvaluateGradient} is {\tt true}
(i.e., {\tt FunctionType} has both {\tt Evaluate()} and {\tt Gradient()}
methods), and {\tt HasEvaluateWithGradient} is {\tt false} (i.e., there is no
{\tt EvaluateWithGradient()} provided by {\tt FunctionType}).  In this
situation, we intend to synthesize an implementation for {\tt
EvaluateWithGradient()} by using both of the provided {\tt Evaluate()} and {\tt
Gradient()} methods sequentially.

\begin{minted}{c++}
template<typename FunctionType, typename MatType, typename GradType>
class AddEvaluateWithGradient<FunctionType, MatType, GradType, true, false>
{
 public:
  // Use FunctionType's Evaluate() and Gradient().
  typename MatType::elem_type EvaluateWithGradient(const MatType& coordinates,
                                                   GradType& gradient)
  {
    const typename MatType::elem_type objective =
        static_cast<Function<FunctionType,
                             MatType, GradType>*>(this)->Evaluate(coordinates);
    static_cast<Function<FunctionType,
                         MatType,
                         GradType>*>(this)->Gradient(coordinates, gradient);
    return objective;
  }
};
\end{minted}

The same complexity with casting is necessary in this specialization too.  The
last specialization is the general case: where there is neither {\tt Evaluate()}
and {\tt Gradient()} nor {\tt EvaluateWithGradient()} provided by {\tt
FunctionType}.  That case is covered by the initial shown implementation of {\tt
AddEvaluateWithGradient}: {\tt EvaluateWithGradient()} is provided with an
argument of type {\tt traits::UnconstructableType}---which is just a class with
a non-public constructor, that cannot be created; and thus, this function cannot
be called.  It is necessary to have this unusable version of {\tt
EvaluateWithGradient()}, though, because the design pattern requires that {\tt
AddEvaluateWithGradient} {\em always} provides a method with the name {\tt
EvaluateWithGradient()}.

This does, however, mean that users can get long and confusing error messages if
the optimizer attempts to instantiate the overload of {\tt
EvaluateWithGradient()} with {\tt traits::UnconstructableType}.  But, remember
that this situation can only happen when using a differentiable optimizer if
the user provided a {\tt FunctionType} that {\it (a)} does not have both {\tt
Evaluate()} or {\tt Gradient()}, or {\it (b)} does not have {\tt
EvaluateWithGradient()}.  This is something that we already have code to
detect---that is the code that computes the values of the boolean template
parameters {\tt HasEvaluateGradient} and {\tt HasEvaluateWithGradient}.
Therefore, we simply use a {\tt static\_assert()} to provide a clear and
understandable error message at compile time when neither of those values are
{\tt true}.  We encapsulate this in a convenient function that can be added at
the beginning of the {\tt Optimize()} method:

% TODO: check that
\begin{minted}{c++}
CheckDifferentiableFunctionType<FunctionType, MatType, GradType>();
\end{minted}

This technique is adapted to each objective function type that {\tt ensmallen}
supports, in order to provide easy error messages of the form

\begin{verbatim}
TODO: good fail output
\end{verbatim}

This is far preferable to the output without {\tt
CheckDifferentiableFunctionType}, which looks like this:

\begin{verbatim}
TODO: bad fail output
\end{verbatim}

The last piece of the puzzle is making sure that when {\tt
Function<...>::EvaluateWithGradient()} is called, that this always calls {\tt
AddEvaluateWithGradient::EvaluateWithGradient()}.  This can be done by a simple
{\tt using} declaration in the body of the {\tt Function} class:

\begin{minted}{c++}
using AddEvaluateWithGradient<FunctionType,
                              MatType,
                              GradType>::EvaluateWithGradient;
\end{minted}

This is all that is needed to infer and synthesize an {\tt
EvaluateWithGradient()} method when the user provided an objective function that
only has {\tt Evaluate()} and {\tt Gradient()}.  There is one detail we have
omitted, though---the code that we have shown here handles non-{\tt const}
and non-{\tt static} versions of methods provided by the user.  Separate
auxiliary structures like {\tt AddEvaluateWithGradientConst} % TODO: check
and {\tt AddEvaluateWithGradientStatic} % TODO: check
are used for function inference in those cases; the general design is identical.

All of these pieces put together result in a clean interface that inference of
methods that users did not provide in their objective function.  Further, this
all happens at {\it compile time} and thus there is no runtime penalty.
Auxiliary structures like {\tt AddEvaluateWithGradient} should be optimized out
by the compiler.

There are more methods than just {\tt EvaluateWithGradient()} that can be
inferred.  We can use the same design strategy to infer numerous other methods:

\begin{itemize}
  \item {\it (Differentiable functions.)}  If the user provides {\tt Evaluate()}
and {\tt Gradient()}, we can synthesize {\tt EvaluateWithGradient()}.

  \item {\it (Differentiable functions.)}  If the user provides {\tt
EvaluateWithGradient()}, we can synthesize {\tt Evaluate()} and/or {\tt
Gradient()}.

  \item {\it (Separable functions.)}  If the user provides {\tt Evaluate()} and
{\tt NumFunctions()}, we can sythesize a non-separable version of {\tt
Evaluate()}.

  \item {\it (Separable functions.)}  If the user provides {\tt Gradient()} and
{\tt NumFunctions()}, we can synthesize a non-separable version of {\tt
Gradient()}.

  \item {\it (Separable functions.)}  If the user provides {\tt
EvaluateWithGradient()} and {\tt NumFunctions()}, we can synthesize a
non-separable version of {\tt Gradient()}.

% TODO: are there more?
% TODO: maybe this is better as a non-comprehensive list?
\end{itemize}
