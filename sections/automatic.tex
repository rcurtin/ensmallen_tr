\subsection{Automatically Inferring Missing Methods}
\label{sec:automatic}

In Section~\ref{sec:linreg_example}, we saw an example of how a user might implement {\tt
Evaluate()} and {\tt Gradient()} for the linear regression objective function
and use {\tt ensmallen} to find the minimum.
However, there is an inefficiency:
the objective function computation is defined as $f(\bm \theta) = \| \bm X \bm \theta - \bm y \|^2$,
and the gradient computation is defined as $f'(\bm \theta) = 2 \bm X^T (\bm X \bm \theta - \bm y)$.
There is a shared inner computation in $f(\bm \theta)$ and $f'(\bm \theta)$: the
term $(\bm X \bm \theta - \bm y)$.
If $f(\bm \theta)$ and $f'(\bm \theta)$ are implemented as separate functions,
there is no easy way to exploit this shared computation.

The differentiable optimizers in {\tt ensmallen} treat the given functions as oracular,
and do not know anything about the internal computations of the functions.
This inefficiency%
\footnote
  {It may be possible to use a specialized implementation of
  auto-differentiation or a programming language with introspection that
  operates directly on the abstract syntax tree~\cite{Jones_AST_2003} of the
  given objective and gradient computations to avoid this inefficiency and
  successfully share the computations across the gradient and objective.
  However, at the time of this writing, we are not aware of any optimization
  packages that actually do this.
  }
can apply to any optimization package that accepts an objective
function and its gradient as separate parameters,
such as SciPy, {\tt Optim.jl} and {\tt bfgsmin()} from Octave~\cite{octave}.

To work around this issue, {\tt ensmallen} internally uses template metaprogramming techniques to allow
the user to provide {\it either} separate implementations of the objective
function and gradient, {\it or} a combined implementation that computes {\it
both} the objective function and gradient simultaneously.
The latter options allows the sharing of inner computations.
That is, the user can provide the methods {\tt Evaluate()} and {\tt Gradient()},
or {\tt EvaluateWithGradient()}.
For the example objective function above,
we empirically show (Section~\ref{sec:experiments}) that the ability to provide
{\tt EvaluateWithGradient()} can result in a significant speedup.
Similarly, when implementing a differentiable optimizer in {\tt ensmallen},
it is possible to use {\it either} {\tt Evaluate()} and {\tt Gradient()},
{\it or} {\tt EvaluateWithGradient()} during optimization.

The same technique is used to infer and provide more missing methods than
just {\tt EvaluateWithGradient()} or {\tt Evaluate()} and {\tt Gradient()}.  
For instance, separable functions, differentiable separable functions, constrained
functions, and categorical functions each have inferrable methods.  Not all of
these possibilities are currently implemented in {\tt ensmallen}, but the
existing framework makes it straightforward to add more.  Below are
examples that are currently implemented:

\begin{itemize}
  \item {\it (Differentiable functions.)}  If the user provides an objective
function with {\tt Evaluate()} and {\tt Gradient()}, we can automatically
synthesize {\tt EvaluateWithGradient()}.

  \item {\it (Differentiable functions.)}  If the user provides an objective
function with {\tt EvaluateWithGradient()}, we can synthesize {\tt Evaluate()}
and/or {\tt Gradient()}.

  \item {\it (Separable functions.)}  If the user provides an objective
function with {\tt Evaluate()} and {\tt NumFunctions()}, we can sythesize a
non-separable version of {\tt Evaluate()}.

  \item {\it (Separable functions.)}  If the user provides an objective function
with {\tt Gradient()} and {\tt NumFunctions()}, we can synthesize a
non-separable version of {\tt Gradient()}.

  \item {\it (Separable functions.)}  If the user provides an objective
function with {\tt EvaluateWithGradient()} and {\tt NumFunctions()}, we can
synthesize a non-separable version of {\tt Gradient()}.
\end{itemize}

For more precise details on exactly how the method generation works,
see Appendix~\ref{sec:automatic_details},
where we describe the framework in a simplified form,
focusing only the {\tt EvaluateWithGradient()}/{\tt Evaluate()}/{\tt Gradient()}
example described above.
