\subsection{Optimization With Various Matrix Types}
\label{sec:templated_optimize}

% We can start by expanding on the linear regression function example to point
% out that we may want different types.

\begin{figure}[t!]
\hrule
\vspace{1ex}
\begin{minted}[fontsize=\small]{c++}
#include <ensmallen.hpp>

// A trivial parabolic function.  The minimum is at the origin.  The function
// works with any matrix type.
class ParabolicFunction
{
 public:
  template<typename MatType>
  typename MatType::elem_type Evaluate(const MatType& coordinates)
  {
    // Return the sum of squared coordinates.
    return arma::accu(arma::square(coordinates));
  }
};

int main()
{
  // Use simulated annealing to optimize the ParabolicFunction.
  ParabolicFunction pf;
  ens::SA<> optimizer(ens::ExponentialSchedule());

  // element type is double
  arma::mat doubleCoordinates(10, 1, arma::fill::randu);
  optimizer.Optimize(pf, doubleCoordinates);

  // element type is float
  arma::fmat floatCoordinates(10, 1, arma::fill::randu);
  optimizer.Optimize(pf, floatCoordinates);

  // element type is int
  arma::imat intCoordinates(10, 1, arma::fill::randi);
  optimizer.Optimize(pf, intCoordinates);
}
\end{minted}
\hrule
\vspace*{-0.5em}
\caption{Example {\tt ensmallen} program showing that {\tt ensmallen}'s
optimizers can be used to seamlessly optimize functions with many different
element types.}
\label{fig:many_optimize}
\end{figure}

\begin{figure}[t!]
\hrule
\vspace{1ex}
\begin{minted}[fontsize=\small]{c++}
class GradientDescent
{
  template<typename FunctionType, typename MatType, typename GradType = MatType>
  typename MatType::elem_type Optimize(FunctionType& function, MatType& coordinates)
  {
    // The step size is hardcoded to 0.01, and the number of iterations is 1000.
    for (size_t i = 0; i < 1000; ++i)
    {
      GradType gradient;
      function.Gradient(coordinates, gradient);

      // Take the step.
      coordinates -= 0.01 * gradient;
    }

    // Compute and return the final objective.
    return function.Evaluate(coordinates);
  }
};
\end{minted}
\hrule
\vspace*{-0.5em}
\caption{Example implementation of a simple gradient descent optimizer.
For the sake of brevity, functionality such as the ability to configure the parameters has
been deliberately omitted.
The actual {\tt GradientDescent} optimizer in {\tt ensmallen} provides more functionality.
}

\label{fig:gd}
\end{figure}

In the example shown in Section~\ref{sec:linreg_example},
where we introduced the class {\tt LinearRegressionFunction},
the matrix and vector objects are hardcoded as {\tt arma::mat} and {\tt arma::vec}.
These objects hold elements with the C++ type {\tt double},
representing double-precision floating point~\cite{Goldberg_CSUR_1991}.
However, in many applications it can be very important to specify a different
underlying element type (e.g. the single-precision {\tt float}, used by {\tt arma::fmat} and {\tt arma::fvec}).
For instance, in the field of machine learning, neural
networks have been shown to be effective with low-precision floating point
representations for weights~\cite{vanhoucke2011improving}.
%% low-precision integers~\cite{courbariaux2015training},
%% or even single bits~\cite{courbariaux2015binaryconnect}.
Furthermore, many optimization problems have parameters
that are best represented as sparse data~\cite{van2011sparse, recht2011hogwild},
which is represented in Armadillo as the {\tt sp\_mat} class~\cite{sanderson2018user, mca24030070}.
Even alternate representations such as data held on the GPU can be quite
important: the use of GPUs can often result in significant
speedups~\cite{oh2004gpu, athanasopoulos2011gpu}.

In order to handle this diverse set of needs, {\tt ensmallen} has been built in
such a way that any underlying storage type can be used to represent the
coordinates to be optimized---so long as it matches the Armadillo API.
This means that the Bandicoot GPU matrix library\footnote{\url{https://gitlab.com/conradsnicta/bandicoot-code}}
can be used as a drop-in replacement once it is stable.

An example of seamlessly using {\tt ensmallen}'s optimizers with different
underlying storage types is given in Figure~\ref{fig:many_optimize}.  In this
example, the types {\tt arma::mat} (which holds {\tt double}), {\tt arma::fmat}
(which holds {\tt float}), and {\tt arma::imat} (which holds {\tt int}) are all
used with {\tt ensmallen}'s simulated annealing implementation to optimize a
very simple parabolic function.  Due to the use of templates both inside {\tt
ensmallen} and in the implementation of the {\tt ParabolicFunction} class, it is
trivial to change the underlying type used for storage.

Consider the simplified gradient descent optimizer shown in Figure~\ref{fig:gd}.
The use of the template types {\tt FunctionType}, {\tt MatType}, and {\tt
GradType} means that at compilation time, the correct types are substituted in
for {\tt FunctionType}, {\tt MatType}, and {\tt GradType} (which by default is
set to be the same as {\tt MatType} in this code).  So long as each type has all
the methods that are used inside of {\tt Optimize()}, there will be no
compilation problems.  Templates are a technique for code generation; in this
case, that means the code generated will be exactly the same as if {\tt
Optimize()} was written with the types specified in those template parameters.
This means that there is no additional runtime overhead when a different {\tt
MatType} is used.

Appendix~\ref{sec:templated_optimize_details} contains details on the internal
compile-time checks used for providing users with concise error messages, 
avoiding the long errors typically associated with template metaprogramming.
